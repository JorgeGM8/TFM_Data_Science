{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a75f5062",
   "metadata": {},
   "source": [
    "# **Preprocesamiento y modelado de datos de viviendas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690f3d01",
   "metadata": {},
   "source": [
    "## **Importación de librerías y carga de datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1996eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import Ridge \n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.base import clone\n",
    "import optuna\n",
    "import pickle\n",
    "import bz2\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc133e7",
   "metadata": {},
   "source": [
    "## **Carga del dataset original (2011–2024)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e7e59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inicial = pd.read_csv(\"../data/final/viviendas_2011_2024.csv\")\n",
    "df_inicial.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4137604",
   "metadata": {},
   "source": [
    "## **Preprocesamiento**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bf23e1",
   "metadata": {},
   "source": [
    "En esta sección se realiza una **limpieza inicial del dataset** para conservar únicamente las variables útiles y mejorar la calidad de los datos:\n",
    "\n",
    "- **Filtrado temporal:** se conservan solo los registros desde el año **2015**.\n",
    "- **Eliminación de columnas con exceso de nulos:** se eliminan `Ano_construccion` y `Ano_reforma`.\n",
    "- **Creación de un indicador de nulos:** se añade `Planta_is_missing` para identificar los casos con valores faltantes en `Planta`.\n",
    "- **Imputación de valores faltantes:** se imputan los valores nulos de `Planta` con la **mediana por distrito**.\n",
    "- **Eliminación de variables altamente correlacionadas:** se retiran variables redundantes que no serán utilizadas en el modelado.\n",
    "- **Verificación final:** se muestran las columnas restantes tras la limpieza.\n",
    "\n",
    "\n",
    "NOTA: Revisar notebook [EDA_viviendas_2011_2024](./EDA_viviendas_2011_2024.ipynb) para visualizar el análisis que da lugar a las decisiones tomadas en el preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b7724f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_inicial.copy()\n",
    "\n",
    "df = df[df['Ano'] >= 2015]\n",
    "\n",
    "# Elminación de Ano_construccion y Ano_reforma por exceso de nulos\n",
    "df = df.drop(columns=['Ano_construccion', 'Ano_reforma'])\n",
    "\n",
    "# Crear columna de indicador de nulos para Planta\n",
    "df['Planta_is_missing'] = df['Planta'].isna().astype(int)\n",
    "\n",
    "# Imputar valores nulos con la mediana para Planta (por distrito)\n",
    "df['Planta'] = df.groupby('Distrito')['Planta'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "# Variables muy correlacionadas y no usadas\n",
    "var_corr = ['Tamano_vivienda_personas', 'Densidad_poblacion', 'Edad_media', 'Renta_neta_persona',\n",
    "            'Renta_neta_hogar', 'Renta_bruta_persona', 'Renta_bruta_hogar', 'Precio_predicho']\n",
    "\n",
    "df = df.drop(columns=var_corr)\n",
    "\n",
    "print('Columnas restantes:', df.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971f403d",
   "metadata": {},
   "source": [
    "En esta etapa se calcula la **tendencia temporal (pendiente lineal)** de diversas variables socioeconómicas y demográficas a lo largo de los años, para cada distrito.\n",
    "\n",
    "**Proceso:**\n",
    "- Se define la función `calcular_tendencia`, que ajusta un **modelo de regresión lineal simple** (`LinearRegression`) entre el año (`Ano`) y cada variable de interés.\n",
    "- Para cada variable:\n",
    "  - Si los valores son **constantes** a lo largo del tiempo, la tendencia se define como `0.0`.\n",
    "  - Si existen suficientes datos y variabilidad en `Ano`, se calcula la **pendiente** del modelo (`coef_[0]`), que representa la tasa de cambio anual.\n",
    "  - Si no hay suficientes datos, se asigna `NaN`.\n",
    "\n",
    "**Variables consideradas:**\n",
    "- `Esperanza_vida`  \n",
    "- `Mayores_65anos%`  \n",
    "- `Menores_18anos%`  \n",
    "- `Paro_registrado%`  \n",
    "- `Apartamentos_turisticos`  \n",
    "- `Superficie_distrito_ha`  \n",
    "- `Zonas_verdes%`\n",
    "\n",
    "El resultado final es un **DataFrame de tendencias por distrito**, donde cada columna indica la **variación anual promedio** de la variable correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ed4d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def calcular_tendencia(grupo, cols):\n",
    "    X = grupo['Ano'].values.reshape(-1, 1)\n",
    "    tendencias = {}\n",
    "    for c in cols:\n",
    "        y = grupo[c].values\n",
    "        if np.allclose(y, y[0], atol=1e-8):  # valores constantes\n",
    "            tendencias[c] = 0.0\n",
    "        elif len(np.unique(X)) > 1:\n",
    "            modelo = LinearRegression().fit(X, y)\n",
    "            tendencias[c] = modelo.coef_[0]\n",
    "        else:\n",
    "            tendencias[c] = np.nan\n",
    "    return pd.Series(tendencias)\n",
    "\n",
    "variables = ['Esperanza_vida', 'Mayores_65anos%',\n",
    "       'Menores_18anos%', 'Paro_registrado%', 'Apartamentos_turisticos',\n",
    "       'Superficie_distrito_ha', 'Zonas_verdes%']\n",
    "cols = ['Ano'] + variables\n",
    "tendencias = df.dropna().groupby('Distrito')[cols].apply(calcular_tendencia, cols=variables).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adddd85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tendencias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6199cf26",
   "metadata": {},
   "source": [
    "Guardado de tendencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b6c8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tendencias.to_csv(\"../data/final/tendencias.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eb6091",
   "metadata": {},
   "source": [
    "## **Imputación de valores faltantes mediante tendencias por distrito**\n",
    "\n",
    "En esta sección se utilizan las **tendencias lineales calculadas previamente** para imputar los valores faltantes de ciertas variables socioeconómicas a nivel de distrito.\n",
    "\n",
    "**Resultado:**  \n",
    "Un dataset sin valores nulos en las variables seleccionadas, imputadas de forma coherente con la evolución temporal observada en cada distrito.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16bfc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renombrar columnas de tendencias para evitar confusión\n",
    "tendencias_rename = {var: f'{var}_tendencia' for var in variables}\n",
    "tendencias = tendencias.rename(columns=tendencias_rename)\n",
    "\n",
    "# Aplicar por distrito (agregado include_groups=False)\n",
    "vars_faltantes = ['Mayores_65anos%', 'Menores_18anos%', 'Paro_registrado%']\n",
    "\n",
    "# Seleccionar solo columna Distrito y las tendencias que necesitamos\n",
    "tendencias_merge = tendencias[['Distrito'] + [f'{var}_tendencia' for var in vars_faltantes]]\n",
    "\n",
    "# Merge limpio\n",
    "df = df.merge(tendencias_merge, on='Distrito', how='left')\n",
    "\n",
    "# Crear indicadores de nulos\n",
    "for var in vars_faltantes:\n",
    "    df[f\"{var}_is_missing\"] = df[var].isna().astype(int)\n",
    "\n",
    "# Función de imputación con tendencia\n",
    "def imputar_con_tendencia(grupo, var):\n",
    "\n",
    "    # Pendiente anual (tasa de cambio por año) a usar para extrapolación futura.\n",
    "    tendencia = grupo[f'{var}_tendencia'].iloc[0]\n",
    "    ultimo_valido = grupo[var].last_valid_index()\n",
    "    \n",
    "    # Si existe al menos un dato observado, extrapola SOLO para años posteriores.\n",
    "    if ultimo_valido is not None:\n",
    "        \n",
    "        # Último valor y su año de referencia (punto base para la extrapolación lineal).\n",
    "        ultimo_valor = grupo.loc[ultimo_valido, var]\n",
    "        ultimo_ano = grupo.loc[ultimo_valido, 'Ano']\n",
    "        \n",
    "        # Imputa valores nulos en años futuros (> último_ano) extrapolando linealmente desde el último valor conocido usando la tendencia.\n",
    "        mask_futuro = grupo[var].isna() & (grupo['Ano'] > ultimo_ano)\n",
    "        grupo.loc[mask_futuro, var] = (ultimo_valor + tendencia * (grupo.loc[mask_futuro, 'Ano'] - ultimo_ano))\n",
    "    \n",
    "    # Relleno final de nulos restantes: ffill/bfill para nulos restantes\n",
    "    return grupo[var].ffill().bfill()\n",
    "\n",
    "for var in vars_faltantes:\n",
    "    df[var] = df.groupby('Distrito', group_keys=False).apply(lambda g: imputar_con_tendencia(g, var), include_groups=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04cfc93",
   "metadata": {},
   "source": [
    "En esta última etapa se realiza una **limpieza posterior a la imputación** y una verificación general del estado del dataset.\n",
    "\n",
    "**Eliminación de columnas auxiliares de tendencia:**  \n",
    "   Las columnas utilizadas para la imputación (`*_tendencia`) ya no son necesarias,  \n",
    "   por lo que se eliminan del DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a259d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar columnas de tendencias que ya no son necesarias tras la imputación\n",
    "cols_tendencia = [col for col in df.columns if '_tendencia' in col]\n",
    "df = df.drop(columns=cols_tendencia)\n",
    "\n",
    "# Verificar estado final del dataset: tipos de datos y ausencia de nulos\n",
    "resumen = pd.DataFrame({\"Tipo\": df.dtypes, \"Nulos\": df.isna().sum(), \"Nulos_%\": (df.isna().mean()*100).round(2)})\n",
    "print(resumen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e695285f",
   "metadata": {},
   "source": [
    "## **Creación de variables temporales y de contexto de mercado**\n",
    "\n",
    "En esta sección se generan **nuevas características (features)** que capturan la **dinámica temporal y espacial del mercado inmobiliario**, con el fin de enriquecer el modelo predictivo.\n",
    "\n",
    "**Resultado:**  \n",
    "Se añaden múltiples **features temporales y de contexto local** que capturan la evolución de los precios, la estabilidad del mercado y la posición relativa de cada vivienda dentro de su entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91c0588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenar el dataframe por las columnas clave para calcular correctamente los lags temporales\n",
    "df = df.sort_values(['Distrito', 'Tipo_vivienda', 'Ano']).reset_index(drop=True)\n",
    "\n",
    "# --- FEATURE 1: Precio promedio año anterior por Distrito + Tipo_vivienda ---\n",
    "# Compara cada vivienda con el precio medio de viviendas similares del año anterior\n",
    "df['precio_prom_ano_ant'] = (df.groupby(['Distrito', 'Tipo_vivienda'])['Precio_ajustado'].shift(1))\n",
    "\n",
    "# --- FEATURE 2: Tendencia de precio por Distrito (cambio porcentual) ---\n",
    "# Captura la dinámica del mercado en cada distrito\n",
    "df['tendencia_distrito'] = (\n",
    "    df.groupby('Distrito')['Precio_ajustado']\n",
    "    .pct_change()  # Variación porcentual respecto al periodo anterior\n",
    ")\n",
    "\n",
    "# --- FEATURE 3: Precio por m² del año anterior por Distrito ---\n",
    "# Normaliza por tamaño para comparar valor relativo\n",
    "df['precio_m2'] = df['Precio_ajustado'] / df['Tamano']\n",
    "df['precio_m2_distrito_ant'] = (\n",
    "    df.groupby('Distrito')['precio_m2']\n",
    "    .shift(1)\n",
    ")\n",
    "\n",
    "# Eliminar columna auxiliar\n",
    "df = df.drop(columns=['precio_m2'])\n",
    "\n",
    "# --- FEATURE 4 (OPCIONAL): Ratio precio vs promedio del distrito ---\n",
    "# Indica si la vivienda está por encima/debajo del mercado local\n",
    "df['precio_prom_distrito'] = df.groupby(['Distrito', 'Ano'])['Precio_ajustado'].transform('mean')\n",
    "df['ratio_vs_distrito'] = df['Precio_ajustado'] / df['precio_prom_distrito']\n",
    "df = df.drop(columns=['precio_prom_distrito'])\n",
    "\n",
    "# --- FEATURE 5 (OPCIONAL): Volatilidad del distrito ---\n",
    "# Mide la variabilidad de precios en cada zona\n",
    "df['volatilidad_distrito'] = (\n",
    "    df.groupby(['Distrito', 'Ano'])['Precio_ajustado']\n",
    "    .transform('std')  # Desviación estándar como proxy de volatilidad\n",
    ")\n",
    "\n",
    "# Imputar nulos resultantes de los lags (primer año de cada grupo)\n",
    "# Usamos mediana por distrito para mantener coherencia espacial\n",
    "for col in ['precio_prom_ano_ant', 'tendencia_distrito', 'precio_m2_distrito_ant', 'volatilidad_distrito']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df.groupby('Distrito')[col].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "print(\"Features temporales agregadas creadas correctamente\")\n",
    "print(f\"Nuevas columnas: {[c for c in df.columns if 'prom_ano' in c or 'tendencia' in c or 'm2_distrito' in c or 'ratio_vs' in c or 'volatilidad' in c]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b10aa59",
   "metadata": {},
   "source": [
    "## **Preparación del dataset y configuración del entrenamiento**\n",
    "\n",
    "En esta sección se preparan los datos y la configuración necesaria para entrenar y evaluar distintos modelos predictivos.  \n",
    "El objetivo es construir un flujo de trabajo **reproducible, limpio y compatible con validación temporal.**\n",
    "\n",
    "**Resultado:**  \n",
    "Dataset y entorno de entrenamiento completamente configurados, con los modelos listos para iniciar la fase de validación y comparación de desempeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa15f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Encoding de variables categóricas ---\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "print(f\"Shape después de get_dummies: {df.shape}\")\n",
    "\n",
    "# --- 2. Preparar dataset ---\n",
    "df = df.sort_values(by=\"Ano\").reset_index(drop=True)\n",
    "df = df.dropna(subset=[\"Precio_ajustado\"])\n",
    "print(f\"Dataset ordenado y limpio: {df.shape}\")\n",
    "\n",
    "# --- 3. Definir X / y ---\n",
    "TARGET_COL = \"Precio_ajustado\"\n",
    "DROP_COLS = [\"Ano\", TARGET_COL]\n",
    "\n",
    "FEATURES = [c for c in df.columns if c not in DROP_COLS]\n",
    "print(f\"Total de features: {len(FEATURES)}\")\n",
    "\n",
    "# --- 4. Separación temporal y por venta y alquiler---\n",
    "mask_train_venta = (df[\"Ano\"] <= 2022) & (df['Operacion_venta'] == 1)\n",
    "mask_train_alquiler = (df[\"Ano\"] <= 2022) & (df['Operacion_venta'] == 0)\n",
    "mask_test_venta = (df[\"Ano\"] >= 2023) & (df['Operacion_venta'] == 1)\n",
    "mask_test_alquiler = (df[\"Ano\"] >= 2023) & (df['Operacion_venta'] == 0)\n",
    "\n",
    "X_train_venta = df.loc[mask_train_venta, FEATURES]\n",
    "y_train_venta = df.loc[mask_train_venta, TARGET_COL]\n",
    "X_train_alquiler = df.loc[mask_train_alquiler, FEATURES]\n",
    "y_train_alquiler = df.loc[mask_train_alquiler, TARGET_COL]\n",
    "\n",
    "X_test_venta = df.loc[mask_test_venta, FEATURES]\n",
    "y_test_venta = df.loc[mask_test_venta, TARGET_COL]\n",
    "X_test_alquiler = df.loc[mask_test_alquiler, FEATURES]\n",
    "y_test_alquiler = df.loc[mask_test_alquiler, TARGET_COL]\n",
    "\n",
    "print(f\"Train venta: {X_train_venta.shape}, Test venta: {X_test_venta.shape}\")\n",
    "print(f\"Train alquiler: {X_train_alquiler.shape}, Test alquiler: {X_test_alquiler.shape}\")\n",
    "\n",
    "# --- 5. Definir pipeline (UNA SOLA VEZ) ---\n",
    "def definir_pipeline(modelo) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Aplica escalado RobustScaler + modelo.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    modelo : estimator\n",
    "        Modelo de sklearn (Ridge, RandomForest, etc.)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pipeline : Pipeline\n",
    "        Pipeline con scaler + modelo\n",
    "    \"\"\"\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', RobustScaler()), \n",
    "        ('modelo', modelo)\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "# --- 6. Función de evaluación ---\n",
    "def evaluar(model, X_tr, y_tr, X_va, y_va):\n",
    "    \"\"\"\n",
    "    Entrena y evalúa un modelo con métricas completas.\n",
    "    \"\"\"\n",
    "    model.fit(X_tr, y_tr)\n",
    "    pred_tr = model.predict(X_tr)\n",
    "    pred_va = model.predict(X_va)\n",
    "\n",
    "    def mape(y_true, y_pred):\n",
    "        eps = 1e-9\n",
    "        return np.mean(np.abs((y_true - y_pred) / (np.abs(y_true) + eps))) * 100\n",
    "\n",
    "    return {\n",
    "        \"MAE_train\": mean_absolute_error(y_tr, pred_tr),\n",
    "        \"RMSE_train\": np.sqrt(mean_squared_error(y_tr, pred_tr)),\n",
    "        \"R2_train\": r2_score(y_tr, pred_tr),\n",
    "        \"MAE_val\": mean_absolute_error(y_va, pred_va),\n",
    "        \"RMSE_val\": np.sqrt(mean_squared_error(y_va, pred_va)),\n",
    "        \"MAPE_val_%\": mape(y_va, pred_va),\n",
    "        \"R2_val\": r2_score(y_va, pred_va),\n",
    "    }\n",
    "\n",
    "# --- 7. Definir modelos ---\n",
    "modelos = {\n",
    "    \"ridge\": definir_pipeline(Ridge(alpha=1.0, random_state=42)),\n",
    "    \"knn\":   definir_pipeline(KNeighborsRegressor(n_neighbors=7, weights=\"distance\", n_jobs=-1)),\n",
    "    \"hgb\":   definir_pipeline(HistGradientBoostingRegressor(loss=\"absolute_error\",\n",
    "                max_depth=15, learning_rate=0.06, max_iter=500, random_state=42)),\n",
    "    \"lgbm\":  definir_pipeline(LGBMRegressor(n_estimators=1000, learning_rate=0.05,\n",
    "                subsample=0.8, colsample_bytree=0.8, force_row_wise=True,\n",
    "                verbosity=-1, n_jobs=-1, random_state=42)),\n",
    "    \"mlp\":   definir_pipeline(MLPRegressor(\n",
    "                hidden_layer_sizes=(64, 32),\n",
    "                max_iter=800,\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=1e-3,\n",
    "                early_stopping=True,\n",
    "                n_iter_no_change=10,\n",
    "                random_state=42)),\n",
    "}\n",
    "\n",
    "# --- 8. Validación temporal con TimeSeriesSplit ---\n",
    "splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=splits)\n",
    "resultados_venta = {}\n",
    "entrenados_venta = {}\n",
    "resultados_alquiler = {}\n",
    "entrenados_alquiler = {}\n",
    "\n",
    "print(\"Setup completado. Listo para entrenar modelos.\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac220790",
   "metadata": {},
   "source": [
    "## **Entrenamiento y validación temporal de modelos**\n",
    "\n",
    "En esta sección se realiza el **entrenamiento con validación temporal** utilizando el método `TimeSeriesSplit`, asegurando que la evaluación de los modelos respete la secuencia cronológica de los datos.\n",
    "\n",
    "**Resultado:**  \n",
    "Se obtienen modelos validados temporalmente, con métricas de desempeño en distintos periodos,  \n",
    "y versiones finales entrenadas con todos los datos históricos disponibles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64392647",
   "metadata": {},
   "source": [
    "**Entrenamientos de prueba para VENTA y ALQUILER:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8771bbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Iniciando entrenamiento con TimeSeriesSplit...\\n\")\n",
    "\n",
    "# Entrenamientos de prueba para VENTA\n",
    "for nombre, pipe in modelos.items():\n",
    "    print(f\"Entrenando modelo: {nombre.upper()}\")\n",
    "    resultados_modelo = []\n",
    "    n_validacion = 0\n",
    "    \n",
    "    for train_index, val_index in tscv.split(X_train_venta):\n",
    "        n_validacion += 1\n",
    "        print(f\"   Split {n_validacion}/{splits}\")\n",
    "\n",
    "        pipe_cv = clone(pipe)\n",
    "\n",
    "        X_tr, X_val = X_train_venta.iloc[train_index], X_train_venta.iloc[val_index]\n",
    "        y_tr, y_val = y_train_venta.iloc[train_index], y_train_venta.iloc[val_index]\n",
    "        \n",
    "        metrics = evaluar(pipe_cv, X_tr, y_tr, X_val, y_val)\n",
    "        resultados_modelo.append(metrics)\n",
    "\n",
    "    resultados_venta[nombre] = resultados_modelo\n",
    "\n",
    "    # Entrenamiento final con todo el train set\n",
    "    print(f\"   Fit final en {X_train_venta.shape[0]} muestras\\n\")\n",
    "    pipe_final = clone(pipe).fit(X_train_venta, y_train_venta)\n",
    "    entrenados_venta[nombre] = pipe_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14615750",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Iniciando entrenamiento con TimeSeriesSplit...\\n\")\n",
    "\n",
    "# Entrenamientos de prueba para ALQUILER\n",
    "for nombre, pipe in modelos.items():\n",
    "    print(f\"Entrenando modelo: {nombre.upper()}\")\n",
    "    resultados_modelo = []\n",
    "    n_validacion = 0\n",
    "    \n",
    "    for train_index, val_index in tscv.split(X_train_alquiler):\n",
    "        n_validacion += 1\n",
    "        print(f\"   Split {n_validacion}/{splits}\")\n",
    "\n",
    "        pipe_cv = clone(pipe)\n",
    "\n",
    "        X_tr, X_val = X_train_alquiler.iloc[train_index], X_train_alquiler.iloc[val_index]\n",
    "        y_tr, y_val = y_train_alquiler.iloc[train_index], y_train_alquiler.iloc[val_index]\n",
    "        \n",
    "        metrics = evaluar(pipe_cv, X_tr, y_tr, X_val, y_val)\n",
    "        resultados_modelo.append(metrics)\n",
    "\n",
    "    resultados_alquiler[nombre] = resultados_modelo\n",
    "\n",
    "    # Entrenamiento final con todo el train set\n",
    "    print(f\"   Fit final en {X_train_alquiler.shape[0]} muestras\\n\")\n",
    "    pipe_final = clone(pipe).fit(X_train_alquiler, y_train_alquiler)\n",
    "    entrenados_alquiler[nombre] = pipe_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc60cd6",
   "metadata": {},
   "source": [
    "**Mostrar resultados de validación cruzada**\n",
    "\n",
    "En esta sección se presentan los **resultados de la validación temporal (2015–2022)** para todos los modelos entrenados.  \n",
    "El objetivo es comparar su desempeño y seleccionar el modelo más prometedor según las métricas de evaluación.\n",
    "\n",
    "Un resumen claro del rendimiento de cada modelo, destacando el mejor candidato para predicciones futuras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5e8a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTADOS DE VALIDACIÓN CRUZADA DE VENTA (2015-2022)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ranking = sorted(\n",
    "    resultados_venta.items(),\n",
    "    key=lambda kv: min([m[\"MAE_val\"] for m in kv[1]])\n",
    ")\n",
    "\n",
    "for nombre, metricas in ranking:\n",
    "    mae_vals = [m[\"MAE_val\"] for m in metricas]\n",
    "    rmse_vals = [m[\"RMSE_val\"] for m in metricas]\n",
    "    r2_vals = [m[\"R2_val\"] for m in metricas]\n",
    "    mape_vals = [m[\"MAPE_val_%\"] for m in metricas]\n",
    "\n",
    "    mae_tr_vals = [m[\"MAE_train\"] for m in metricas]\n",
    "    rmse_tr_vals = [m[\"RMSE_train\"] for m in metricas]\n",
    "    r2_tr_vals = [m[\"R2_train\"] for m in metricas]\n",
    "\n",
    "    print(f\"\\n{nombre.upper()}:\")\n",
    "    print(f\"  Validación:\")\n",
    "    print(f\"    MAE    = {min(mae_vals):,.2f} EUR\")\n",
    "    print(f\"    RMSE   = {min(rmse_vals):,.2f} EUR\")\n",
    "    print(f\"    R2     = {max(r2_vals):.4f}\")\n",
    "    print(f\"    MAPE   = {min(mape_vals):.2f}%\")\n",
    "    print(f\"  Entrenamiento:\")\n",
    "    print(f\"    MAE    = {min(mae_tr_vals):,.2f} EUR\")\n",
    "    print(f\"    RMSE   = {min(rmse_tr_vals):,.2f} EUR\")\n",
    "    print(f\"    R2     = {max(r2_tr_vals):.4f}\")\n",
    "\n",
    "mejor_nombre_venta = ranking[0][0]\n",
    "mejor_modelo_venta = entrenados_venta[mejor_nombre_venta]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"MEJOR MODELO: {mejor_nombre_venta.upper()}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTADOS DE VALIDACIÓN CRUZADA DE ALQUILER (2015-2022)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ranking = sorted(\n",
    "    resultados_alquiler.items(),\n",
    "    key=lambda kv: min([m[\"MAE_val\"] for m in kv[1]])\n",
    ")\n",
    "\n",
    "for nombre, metricas in ranking:\n",
    "    mae_vals = [m[\"MAE_val\"] for m in metricas]\n",
    "    rmse_vals = [m[\"RMSE_val\"] for m in metricas]\n",
    "    r2_vals = [m[\"R2_val\"] for m in metricas]\n",
    "    mape_vals = [m[\"MAPE_val_%\"] for m in metricas]\n",
    "\n",
    "    mae_tr_vals = [m[\"MAE_train\"] for m in metricas]\n",
    "    rmse_tr_vals = [m[\"RMSE_train\"] for m in metricas]\n",
    "    r2_tr_vals = [m[\"R2_train\"] for m in metricas]\n",
    "\n",
    "    print(f\"\\n{nombre.upper()}:\")\n",
    "    print(f\"  Validación:\")\n",
    "    print(f\"    MAE    = {min(mae_vals):,.2f} EUR\")\n",
    "    print(f\"    RMSE   = {min(rmse_vals):,.2f} EUR\")\n",
    "    print(f\"    R2     = {max(r2_vals):.4f}\")\n",
    "    print(f\"    MAPE   = {min(mape_vals):.2f}%\")\n",
    "    print(f\"  Entrenamiento:\")\n",
    "    print(f\"    MAE    = {min(mae_tr_vals):,.2f} EUR\")\n",
    "    print(f\"    RMSE   = {min(rmse_tr_vals):,.2f} EUR\")\n",
    "    print(f\"    R2     = {max(r2_tr_vals):.4f}\")\n",
    "\n",
    "mejor_nombre_alquiler = ranking[0][0]\n",
    "mejor_modelo_alquiler = entrenados_alquiler[mejor_nombre_alquiler]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"MEJOR MODELO: {mejor_nombre_alquiler.upper()}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1e7f7d",
   "metadata": {},
   "source": [
    "## **Optimización de hiperparámetros con Optuna (Random Forest)**\n",
    "\n",
    "En esta sección se realiza la **optimización automática de hiperparámetros** del mejor modelo utilizando la librería **Optuna**, con el objetivo de minimizar el error medio absoluto (MAE) en validación temporal.\n",
    "\n",
    "**Resultado:**  \n",
    "Un modelo optimizado mediante búsqueda automática de hiperparámetros,  \n",
    "validado temporalmente y listo para compararse con los demás modelos base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a54794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lgbm_venta(trial):\n",
    "    \"\"\"\n",
    "    Función objetivo para Optuna: minimiza MAE promedio en validación cruzada.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"mae\",\n",
    "        \"verbose\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 1000, step=100),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.05, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 15, 255),\n",
    "        \"max_depth\": -1,\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100, step=5),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "        \"min_split_gain\": trial.suggest_float(\"min_split_gain\", 0.0, 0.3),\n",
    "        \"feature_fraction_bynode\": trial.suggest_float(\"feature_fraction_bynode\", 0.6, 1.0),\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1,\n",
    "    }\n",
    "\n",
    "    pipe = definir_pipeline(LGBMRegressor(**params))\n",
    "\n",
    "    maes = []\n",
    "    for train_idx, val_idx in tscv.split(X_train_venta):\n",
    "        pipe_cv = clone(pipe)\n",
    "\n",
    "        X_tr, X_val = X_train_venta.iloc[train_idx], X_train_venta.iloc[val_idx]\n",
    "        y_tr, y_val = y_train_venta.iloc[train_idx], y_train_venta.iloc[val_idx]\n",
    "        \n",
    "        mets = evaluar(pipe_cv, X_tr, y_tr, X_val, y_val)\n",
    "        maes.append(mets[\"MAE_val\"])\n",
    "\n",
    "    return np.mean(maes)\n",
    "\n",
    "def objective_lgbm_alquiler(trial):\n",
    "    \"\"\"\n",
    "    Función objetivo para Optuna: minimiza MAE promedio en validación cruzada.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"mae\",\n",
    "        \"verbose\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 1000, step=100),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 15, 255),\n",
    "        \"max_depth\": -1,\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100, step=5),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "        \"min_split_gain\": trial.suggest_float(\"min_split_gain\", 0.0, 0.3),\n",
    "        \"feature_fraction_bynode\": trial.suggest_float(\"feature_fraction_bynode\", 0.6, 1.0),\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1,\n",
    "    }\n",
    "\n",
    "    pipe = definir_pipeline(LGBMRegressor(**params))\n",
    "\n",
    "    maes = []\n",
    "    for train_idx, val_idx in tscv.split(X_train_alquiler):\n",
    "        pipe_cv = clone(pipe)\n",
    "\n",
    "        X_tr, X_val = X_train_alquiler.iloc[train_idx], X_train_alquiler.iloc[val_idx]\n",
    "        y_tr, y_val = y_train_alquiler.iloc[train_idx], y_train_alquiler.iloc[val_idx]\n",
    "        \n",
    "        mets = evaluar(pipe_cv, X_tr, y_tr, X_val, y_val)\n",
    "        maes.append(mets[\"MAE_val\"])\n",
    "\n",
    "    return np.mean(maes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ebec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nIniciando optimización de hiperparámetros para VENTA con Optuna...\")\n",
    "print(f\"Usando TimeSeriesSplit con {splits} splits\\n\")\n",
    "\n",
    "# Crear estudio de Optuna para VENTA\n",
    "study_venta = optuna.create_study(direction=\"minimize\", study_name=\"Optimizacion_LGBM_venta\")\n",
    "study_venta.optimize(objective_lgbm_venta, n_trials=40, show_progress_bar=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTADOS DE OPTIMIZACIÓN\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Mejor MAE validación: {study_venta.best_value:,.2f} EUR\")\n",
    "print(f\"\\nMejores hiperparámetros:\")\n",
    "for param, value in study_venta.best_params.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5af203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nIniciando optimización de hiperparámetros para ALQUILER con Optuna...\")\n",
    "print(f\"Usando TimeSeriesSplit con {splits} splits\\n\")\n",
    "\n",
    "# Crear estudio de Optuna para ALQUILER\n",
    "study_alquiler = optuna.create_study(direction=\"minimize\", study_name=\"Optimizacion_LGBM_alquiler\")\n",
    "study_alquiler.optimize(objective_lgbm_alquiler, n_trials=40, show_progress_bar=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTADOS DE OPTIMIZACIÓN\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Mejor MAE validación: {study_alquiler.best_value:,.2f} EUR\")\n",
    "print(f\"\\nMejores hiperparámetros:\")\n",
    "for param, value in study_alquiler.best_params.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714e6a1e",
   "metadata": {},
   "source": [
    "**Entrenamiento del modelo Random Forest final con los mejores hiperparámetros**\n",
    "\n",
    "Se entrena el modelo Random Forest utilizando los hiperparámetros óptimos encontrados con Optuna. El modelo final se integra en un pipeline con escalado robusto y se ajusta sobre los datos de entrenamiento completos, obteniendo así la versión definitiva lista para evaluación y predicciones futuras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c184701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Entrenar el mejor modelo final =====\n",
    "lgbm_final_venta = definir_pipeline(LGBMRegressor(**study_venta.best_params, n_jobs=-1, random_state=42))\n",
    "lgbm_final_alquiler = definir_pipeline(LGBMRegressor(**study_alquiler.best_params, random_state=42))\n",
    "\n",
    "# Entrenar con todos los datos (ya que es una serie temporal)\n",
    "lgbm_final_venta.fit(X_train_venta, y_train_venta)\n",
    "lgbm_final_alquiler.fit(X_train_alquiler, y_train_alquiler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1202b555",
   "metadata": {},
   "source": [
    "Guardar modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b4c2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.open('../models/lgbm_final_venta.pkl.bz2', 'wb') as f:\n",
    "    pickle.dump(lgbm_final_venta, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fb841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.open('../models/lgbm_final_alquiler.pkl.bz2', 'wb') as f:\n",
    "    pickle.dump(lgbm_final_alquiler, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9b7046",
   "metadata": {},
   "source": [
    "Cargar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8180ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.open('../models/lgbm_final_venta.pkl.bz2', 'rb') as f:\n",
    "    lgbm_final_venta = pickle.load(lgbm_final_venta, f)\n",
    "with bz2.open('../models/lgbm_final_alquiler.pkl.bz2', 'rb') as f:\n",
    "    lgbm_final_alquiler = pickle.load(lgbm_final_alquiler, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6caca0",
   "metadata": {},
   "source": [
    "**Análisis de la importancia de variables en el modelo Random Forest**\n",
    "\n",
    "Se extraen y visualizan las variables con mayor peso en el modelo final de Random Forest. Este análisis permite identificar los factores que más influyen en la predicción del precio ajustado, mostrando las 15 variables más relevantes mediante una tabla y un gráfico de barras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1df238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extraer importancias\n",
    "importancias = rf_final.named_steps['modelo'].feature_importances_\n",
    "features = X_train.columns\n",
    "\n",
    "# Crear DataFrame ordenado\n",
    "imp_df = pd.DataFrame({\n",
    "    'Variable': features,\n",
    "    'Importancia': importancias\n",
    "}).sort_values(by='Importancia', ascending=False).head(15)\n",
    "\n",
    "# Mostrar tabla y gráfico\n",
    "display(imp_df)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.barh(imp_df['Variable'], imp_df['Importancia'])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Top 15 variables más importantes en el modelo Random Forest')\n",
    "plt.xlabel('Importancia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab59006",
   "metadata": {},
   "source": [
    "Evaluación en 2023 y 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034c7f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar que X_test no tenga nulos\n",
    "print(\"Nulos en X_test:\")\n",
    "print(X_test.isna().sum().sum())\n",
    "\n",
    "# Si no hay nulos, evaluar directamente\n",
    "y_pred_test = rf_final.predict(X_test)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUACIÓN FINAL EN TEST (2023-2024)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"MAE:  {mean_absolute_error(y_test, y_pred_test):.2f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.2f}\")\n",
    "print(f\"R²:   {r2_score(y_test, y_pred_test):.4f}\")\n",
    "\n",
    "# MAPE\n",
    "mape_test = np.mean(np.abs((y_test - y_pred_test) / (np.abs(y_test) + 1e-9))) * 100\n",
    "print(f\"MAPE: {mape_test:.2f}%\")\n",
    "\n",
    "# Comparación visual\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_test, alpha=0.5, edgecolors='k', linewidth=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Predicción perfecta')\n",
    "plt.xlabel('Precio Real', fontsize=12)\n",
    "plt.ylabel('Precio Predicho', fontsize=12)\n",
    "plt.title('Predicciones vs Realidad (Test 2023-2024)', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6840afc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de errores\n",
    "errores = y_test - y_pred_test\n",
    "errores_abs = np.abs(errores)\n",
    "errores_rel = np.abs(errores / y_test) * 100\n",
    "\n",
    "print(\"Análisis de errores:\")\n",
    "print(f\"Error medio: {errores.mean():.2f} (sesgo)\")\n",
    "print(f\"Mediana del error absoluto: {np.median(errores_abs):.2f}\")\n",
    "print(f\"Percentil 90 del error relativo: {np.percentile(errores_rel, 90):.2f}%\")\n",
    "print(f\"% predicciones con error <10%: {(errores_rel < 10).mean() * 100:.1f}%\")\n",
    "print(f\"% predicciones con error >50%: {(errores_rel > 50).mean() * 100:.1f}%\")\n",
    "\n",
    "# Identificar peores predicciones\n",
    "peores = pd.DataFrame({\n",
    "    'Real': y_test,\n",
    "    'Predicho': y_pred_test,\n",
    "    'Error_abs': errores_abs,\n",
    "    'Error_rel_%': errores_rel\n",
    "}).sort_values('Error_abs', ascending=False).head(10)\n",
    "\n",
    "print(\"\\nPeores 10 predicciones:\")\n",
    "display(peores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ad033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear rangos de precio\n",
    "bins = [0, 300000, 600000, 1000000, 2000000, np.inf]\n",
    "labels = ['<300k', '300-600k', '600k-1M', '1-2M', '>2M']\n",
    "\n",
    "df_errores = pd.DataFrame({\n",
    "    'Real': y_test.values,\n",
    "    'Predicho': y_pred_test,\n",
    "    'Error_abs': errores_abs,\n",
    "    'Error_rel_%': errores_rel\n",
    "})\n",
    "\n",
    "df_errores['Rango'] = pd.cut(df_errores['Real'], bins=bins, labels=labels)\n",
    "\n",
    "# Análisis por rango\n",
    "print(\"Rendimiento por rango de precio:\\n\")\n",
    "resumen_rangos = df_errores.groupby('Rango').agg({\n",
    "    'Error_abs': ['mean', 'median'],\n",
    "    'Error_rel_%': ['mean', 'median'],\n",
    "    'Real': 'count'\n",
    "}).round(2)\n",
    "\n",
    "resumen_rangos.columns = ['MAE', 'Mediana_error', 'Error_rel_%_medio', 'Error_rel_%_mediano', 'n_casos']\n",
    "display(resumen_rangos)\n",
    "\n",
    "# Visualizar errores por rango\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "df_errores.boxplot(column='Error_rel_%', by='Rango', ax=plt.gca())\n",
    "plt.title('Distribución del error relativo por rango de precio')\n",
    "plt.ylabel('Error relativo (%)')\n",
    "plt.xlabel('Rango de precio')\n",
    "plt.suptitle('')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "resumen_rangos['Error_rel_%_medio'].plot(kind='bar', color='steelblue')\n",
    "plt.title('Error relativo promedio por rango')\n",
    "plt.ylabel('Error relativo medio (%)')\n",
    "plt.xlabel('Rango de precio')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f24993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar casos problemáticos en <300k\n",
    "df_bajo = df_errores[df_errores['Rango'] == '<300k']\n",
    "casos_extremos = df_bajo[df_bajo['Error_rel_%'] > 100]\n",
    "\n",
    "print(f\"Casos con error >100% en segmento <300k: {len(casos_extremos)} ({len(casos_extremos)/len(df_bajo)*100:.2f}%)\")\n",
    "print(f\"\\nEstadísticas de estos casos extremos:\")\n",
    "print(f\"Precio real promedio: {casos_extremos['Real'].mean():.2f}\")\n",
    "print(f\"Precio predicho promedio: {casos_extremos['Predicho'].mean():.2f}\")\n",
    "\n",
    "# Obtener índices para investigar features\n",
    "indices_extremos = casos_extremos.index.tolist()[:5]\n",
    "print(f\"\\nPrimeros 5 casos extremos (índices): {indices_extremos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e854303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_sin_fit(model, X_tr, y_tr, X_va, y_va):\n",
    "    pred_tr = model.predict(X_tr)\n",
    "    pred_va = model.predict(X_va)\n",
    "\n",
    "    def mape(y_true, y_pred):\n",
    "        eps = 1e-9\n",
    "        return np.mean(np.abs((y_true - y_pred) / (np.abs(y_true) + eps))) * 100\n",
    "\n",
    "    return {\n",
    "        \"MAE_train\": mean_absolute_error(y_tr, pred_tr),\n",
    "        \"RMSE_train\": np.sqrt(mean_squared_error(y_tr, pred_tr)),\n",
    "        \"R2_train\": r2_score(y_tr, pred_tr),\n",
    "        \"MAE_val\": mean_absolute_error(y_va, pred_va),\n",
    "        \"RMSE_val\": np.sqrt(mean_squared_error(y_va, pred_va)),\n",
    "        \"MAPE_val_%\": mape(y_va, pred_va),\n",
    "        \"R2_val\": r2_score(y_va, pred_va),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102f3933",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Evaluación en 2023 ---\")\n",
    "eval_2023 = evaluar_sin_fit(rf_final, X_train, y_train,\n",
    "                    X_test.loc[df[\"Ano\"] == 2023],\n",
    "                    y_test.loc[df[\"Ano\"] == 2023])\n",
    "print(eval_2023)\n",
    "\n",
    "print(\"\\n--- Evaluación en 2024 ---\")\n",
    "eval_2024 = evaluar_sin_fit(rf_final, X_train, y_train,\n",
    "                    X_test.loc[df[\"Ano\"] == 2024],\n",
    "                    y_test.loc[df[\"Ano\"] == 2024])\n",
    "print(eval_2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd6ad11",
   "metadata": {},
   "source": [
    "**Comparativa de desempeño entre el modelo base y el modelo optimizado**\n",
    "\n",
    "Se comparan los resultados del modelo Random Forest inicial frente al modelo optimizado tras la búsqueda de hiperparámetros. El modelo ajustado muestra una mejora en el MAE y MAPE de validación, manteniendo un R² alto, lo que confirma una mayor precisión y mejor capacidad de generalización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116c6699",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_base = entrenados[\"rf\"]  # el modelo base antes del tuning\n",
    "\n",
    "base_mets = resultados[\"rf\"]\n",
    "tuned_mets = evaluar(rf_final, X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(\"\\n--- Comparativa Random Forest ---\")\n",
    "print(\"Base :\", base_mets)\n",
    "print(\"Tuned:\", tuned_mets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0b2f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarnos de usar todo el test set\n",
    "y_pred_test_final = rf_final.predict(X_test)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MÉTRICAS FINALES EN TEST COMPLETO (2023-2024)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"MAE:  {mean_absolute_error(y_test, y_pred_test_final):,.2f} EUR\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test_final)):,.2f} EUR\")\n",
    "print(f\"R²:   {r2_score(y_test, y_pred_test_final):.4f}\")\n",
    "\n",
    "mape_final = np.mean(np.abs((y_test - y_pred_test_final) / (np.abs(y_test) + 1e-9))) * 100\n",
    "print(f\"MAPE: {mape_final:.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFM (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
